{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff725da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib import request, parse, robotparser, error\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4d427",
   "metadata": {},
   "source": [
    "## Scrap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC Food Recipe Scraper\n",
      "----------------------------------------\n",
      "[1/89] https://www.bbc.co.uk/food/collections/1_best_sausage_recipes\n",
      "[2/89] https://www.bbc.co.uk/food/collections/african_inspired_food\n",
      "[3/89] https://www.bbc.co.uk/food/collections/air_fryer_family_food\n",
      "[4/89] https://www.bbc.co.uk/food/collections/al-desko\n",
      "[5/89] https://www.bbc.co.uk/food/collections/back_to_basics\n",
      "[6/89] https://www.bbc.co.uk/food/collections/baked_cheesecakes\n",
      "[7/89] https://www.bbc.co.uk/food/collections/baking_with_vegetables\n",
      "[8/89] https://www.bbc.co.uk/food/collections/barbecue_roasts\n",
      "[9/89] https://www.bbc.co.uk/food/collections/camping_recipes\n",
      "[10/89] https://www.bbc.co.uk/food/collections/caribbean_barbecue_recipes\n",
      "[11/89] https://www.bbc.co.uk/food/collections/celebration_cakes\n",
      "[12/89] https://www.bbc.co.uk/food/collections/cheap_chicken_dinners_for_four\n",
      "[13/89] https://www.bbc.co.uk/food/collections/dairy-free_baking\n",
      "[14/89] https://www.bbc.co.uk/food/collections/dark_chocolate_treats\n",
      "[15/89] https://www.bbc.co.uk/food/collections/date_night_recipes\n",
      "[16/89] https://www.bbc.co.uk/food/collections/delicious_dal_recipes\n",
      "[17/89] https://www.bbc.co.uk/food/collections/dinner_on_the_go\n",
      "[18/89] https://www.bbc.co.uk/food/collections/easter_mains\n",
      "[19/89] https://www.bbc.co.uk/food/collections/easy_classics\n",
      "[20/89] https://www.bbc.co.uk/food/collections/eid_feasts\n",
      "[21/89] https://www.bbc.co.uk/food/collections/english_baking_recipes\n",
      "[22/89] https://www.bbc.co.uk/food/collections/family_friendly_dinner\n",
      "[23/89] https://www.bbc.co.uk/food/collections/super-food_salads\n",
      "[24/89] https://www.bbc.co.uk/food/collections/france\n",
      "[25/89] https://www.bbc.co.uk/food/collections/fried_favourites\n",
      "[26/89] https://www.bbc.co.uk/food/collections/gluten-free_bread_and_cake_recipes\n",
      "[27/89] https://www.bbc.co.uk/food/collections/go_green\n",
      "[28/89] https://www.bbc.co.uk/food/collections/good_mood_food\n",
      "[29/89] https://www.bbc.co.uk/food/collections/ginger_spice\n",
      "[30/89] https://www.bbc.co.uk/food/collections/halloween_baking_recipes\n",
      "[31/89] https://www.bbc.co.uk/food/collections/healthy_fast_foodie\n",
      "[32/89] https://www.bbc.co.uk/food/collections/healthy_budget_breakfasts\n",
      "[33/89] https://www.bbc.co.uk/food/collections/high_protein_dinners\n",
      "[34/89] https://www.bbc.co.uk/food/collections/indian_family_feasts\n",
      "[35/89] https://www.bbc.co.uk/food/collections/italian_comfort_food\n",
      "[36/89] https://www.bbc.co.uk/food/collections/instagram_recipes\n",
      "[37/89] https://www.bbc.co.uk/food/collections/japanese_flavours\n",
      "[38/89] https://www.bbc.co.uk/food/collections/jamaican_flavours\n",
      "[39/89] https://www.bbc.co.uk/food/collections/january_recipes\n",
      "[40/89] https://www.bbc.co.uk/food/collections/korean_kitchen\n",
      "[41/89] https://www.bbc.co.uk/food/collections/kids_holiday_activity_recipes\n",
      "[42/89] https://www.bbc.co.uk/food/collections/kimchi_favourites\n",
      "[43/89] https://www.bbc.co.uk/food/collections/leftovers\n",
      "[44/89] https://www.bbc.co.uk/food/collections/light_dinner_ideas\n",
      "[45/89] https://www.bbc.co.uk/food/collections/low_fodmap_recipes\n",
      "[46/89] https://www.bbc.co.uk/food/collections/5_lower-carb_puds\n",
      "[47/89] https://www.bbc.co.uk/food/collections/meal_prep_pasta_salad\n",
      "[48/89] https://www.bbc.co.uk/food/collections/meals_for_one\n",
      "[49/89] https://www.bbc.co.uk/food/collections/mediterranean_family_dinners\n",
      "[50/89] https://www.bbc.co.uk/food/collections/mexican_chicken_recipes\n",
      "[51/89] https://www.bbc.co.uk/food/collections/movie_night_snacks\n",
      "[52/89] https://www.bbc.co.uk/food/collections/noodle_bowls\n",
      "[53/89] https://www.bbc.co.uk/food/collections/nutty_comfort_food\n",
      "[54/89] https://www.bbc.co.uk/food/collections/november_recipes\n",
      "[55/89] https://www.bbc.co.uk/food/collections/cakes_made_with_nuts\n",
      "[56/89] https://www.bbc.co.uk/food/collections/one-pot_pasta_recipes\n",
      "[57/89] https://www.bbc.co.uk/food/collections/overnight_oats\n",
      "[58/89] https://www.bbc.co.uk/food/collections/one_dish_dinners\n",
      "[59/89] https://www.bbc.co.uk/food/collections/pancakes_for_dinner\n",
      "[60/89] https://www.bbc.co.uk/food/collections/pasta_with_a_twist\n",
      "[61/89] https://www.bbc.co.uk/food/collections/pressure_cooker_recipes\n",
      "[62/89] https://www.bbc.co.uk/food/collections/special_pasta_recipes\n",
      "[63/89] https://www.bbc.co.uk/food/collections/quick_and_cheap_dinners\n",
      "[64/89] https://www.bbc.co.uk/food/collections/quick_curries\n",
      "[65/89] https://www.bbc.co.uk/food/collections/quick_italian\n",
      "[66/89] https://www.bbc.co.uk/food/collections/8_quality_quiches\n",
      "[67/89] https://www.bbc.co.uk/food/collections/raw_snacks\n",
      "[68/89] https://www.bbc.co.uk/food/collections/retro_british_recipes\n",
      "[69/89] https://www.bbc.co.uk/food/collections/romantic_recipes\n",
      "[70/89] https://www.bbc.co.uk/food/collections/russian_recipes\n",
      "[71/89] https://www.bbc.co.uk/food/collections/salads_to_go\n",
      "[72/89] https://www.bbc.co.uk/food/collections/seriously_fast_food\n",
      "[73/89] https://www.bbc.co.uk/food/collections/some_like_it_hot\n",
      "[74/89] https://www.bbc.co.uk/food/collections/sri_lankan_feasts\n",
      "[75/89] https://www.bbc.co.uk/food/collections/sweet_tarts\n",
      "[76/89] https://www.bbc.co.uk/food/collections/takeaway_dinners\n",
      "[77/89] https://www.bbc.co.uk/food/collections/thai_noodle_recipes\n",
      "[78/89] https://www.bbc.co.uk/food/collections/top_soup_recipes\n",
      "[79/89] https://www.bbc.co.uk/food/collections/tv_dinners\n",
      "[80/89] https://www.bbc.co.uk/food/collections/using_curry_paste\n",
      "[81/89] https://www.bbc.co.uk/food/collections/using_frozen_ingredients\n",
      "[82/89] https://www.bbc.co.uk/food/collections/using_tomatoes\n",
      "[83/89] https://www.bbc.co.uk/food/collections/6_vegan_bakes\n",
      "[84/89] https://www.bbc.co.uk/food/collections/vegan_barbecue\n",
      "[85/89] https://www.bbc.co.uk/food/collections/vegetarian_air_fryer_recipes\n",
      "[86/89] https://www.bbc.co.uk/food/collections/veggie_comfort_food\n",
      "[87/89] https://www.bbc.co.uk/food/collections/warm_and_filling_salads\n",
      "[88/89] https://www.bbc.co.uk/food/collections/weekend_cooking_projects\n",
      "[89/89] https://www.bbc.co.uk/food/collections/weeknight_dinners_for_two\n",
      "Done! Wrote 445 recipes to bbc_food_recipes.csv and bbc_food_recipes.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ---------- Configuration ----------\n",
    "BASE = \"https://www.bbc.co.uk\"\n",
    "UA = \"Mozilla/5.0 (compatible; RecipeScraper/1.0)\"\n",
    "\n",
    "# ---------- HTTP ----------\n",
    "def fetch(url, timeout=20, sleep=0.8):\n",
    "    \"\"\"Fetch URL and return bytes.\"\"\"\n",
    "    req = request.Request(url, headers={\"User-Agent\": UA})\n",
    "    try:\n",
    "        with request.urlopen(req, timeout=timeout) as resp:\n",
    "            time.sleep(sleep)\n",
    "            return resp.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Parsing ----------\n",
    "def get_links(html, pattern):\n",
    "    \"\"\"Extract unique links matching pattern.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = set()\n",
    "    for a in soup.select(f'a[href^=\"{pattern}\"]'):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if href.startswith(pattern) and \"?\" not in href and \"#\" not in href:\n",
    "            links.add(parse.urljoin(BASE, href))\n",
    "    return sorted(links)\n",
    "\n",
    "def get_next_page(html):\n",
    "    \"\"\"Find next page link.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a = soup.find(\"a\", attrs={\"rel\": \"next\"})\n",
    "    if a and a.get(\"href\"):\n",
    "        return parse.urljoin(BASE, a[\"href\"])\n",
    "    return None\n",
    "\n",
    "def iso_to_minutes(iso_str):\n",
    "    \"\"\"Convert ISO 8601 duration to minutes.\"\"\"\n",
    "    m = re.match(r\"^P(?:(\\d+)D)?(?:T(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?)?$\", iso_str.strip(), re.I)\n",
    "    if not m:\n",
    "        return None\n",
    "    days, hours, mins, secs = [int(x or 0) for x in m.groups()]\n",
    "    return days*1440 + hours*60 + mins + (1 if secs and not (mins or hours) else 0)\n",
    "\n",
    "def extract_text(obj):\n",
    "    \"\"\"Recursively extract text from JSON-LD objects.\"\"\"\n",
    "    if not obj:\n",
    "        return None\n",
    "    if isinstance(obj, list):\n",
    "        parts = [extract_text(x) for x in obj]\n",
    "        return \" || \".join(p for p in parts if p) or None\n",
    "    if isinstance(obj, dict):\n",
    "        if \"text\" in obj:\n",
    "            return extract_text(obj[\"text\"])\n",
    "        parts = [extract_text(v) for k, v in obj.items() if not k.startswith(\"@\")]\n",
    "        return \" \".join(p for p in parts if p) or None\n",
    "    return str(obj).strip() or None\n",
    "\n",
    "def find_recipe_jsonld(html):\n",
    "    \"\"\"Find Recipe JSON-LD in HTML.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for script in soup.find_all(\"script\", type=lambda v: v and \"ld+json\" in v.lower()):\n",
    "        try:\n",
    "            data = json.loads(script.string or script.get_text())\n",
    "        except:\n",
    "            continue\n",
    "        candidates = data if isinstance(data, list) else [data]\n",
    "        for obj in candidates:\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "            obj_type = obj.get(\"@type\", \"\")\n",
    "            if isinstance(obj_type, list):\n",
    "                obj_type = \" \".join(str(t).lower() for t in obj_type)\n",
    "            if \"recipe\" in str(obj_type).lower():\n",
    "                return obj\n",
    "            if \"@graph\" in obj:\n",
    "                for g in obj[\"@graph\"]:\n",
    "                    if isinstance(g, dict):\n",
    "                        g_type = str(g.get(\"@type\", \"\")).lower()\n",
    "                        if \"recipe\" in g_type:\n",
    "                            return g\n",
    "    return None\n",
    "\n",
    "def parse_recipe(html):\n",
    "    \"\"\"Extract recipe data from HTML.\"\"\"\n",
    "    rj = find_recipe_jsonld(html)\n",
    "    if not rj:\n",
    "        return {}\n",
    "    data = {\n",
    "        \"title\": extract_text(rj.get(\"name\")),\n",
    "        \"description\": extract_text(rj.get(\"description\")),\n",
    "        \"servings\": extract_text(rj.get(\"recipeYield\")),\n",
    "        \"ingredients\": extract_text(rj.get(\"recipeIngredient\")),\n",
    "    }\n",
    "    total_time = rj.get(\"totalTime\")\n",
    "    if not total_time:\n",
    "        prep = rj.get(\"prepTime\")\n",
    "        cook = rj.get(\"cookTime\")\n",
    "        if prep or cook:\n",
    "            mins = (iso_to_minutes(prep) or 0) + (iso_to_minutes(cook) or 0)\n",
    "            total_time = f\"PT{mins}M\" if mins else None\n",
    "    data[\"total_time_min\"] = iso_to_minutes(total_time) if total_time else None\n",
    "    instr = rj.get(\"recipeInstructions\", [])\n",
    "    if isinstance(instr, list):\n",
    "        steps = []\n",
    "        for item in instr:\n",
    "            if isinstance(item, dict) and \"itemListElement\" in item:\n",
    "                steps.extend(extract_text(x) for x in item[\"itemListElement\"])\n",
    "            else:\n",
    "                steps.append(extract_text(item))\n",
    "        data[\"method\"] = \" || \".join(s for s in steps if s) or None\n",
    "    else:\n",
    "        data[\"method\"] = extract_text(instr)\n",
    "    tags = []\n",
    "    for key in (\"keywords\", \"recipeCategory\", \"recipeCuisine\", \"suitableForDiet\"):\n",
    "        val = rj.get(key)\n",
    "        if isinstance(val, str):\n",
    "            tags.extend(p.strip() for p in val.split(\",\") if p.strip())\n",
    "        elif isinstance(val, list):\n",
    "            tags.extend(extract_text(v) for v in val if extract_text(v))\n",
    "    data[\"tags\"] = \" | \".join(t for t in tags if t) or None\n",
    "    agg = rj.get(\"aggregateRating\", {})\n",
    "    data[\"ratingValue\"] = agg.get(\"ratingValue\")\n",
    "    data[\"ratingCount\"] = agg.get(\"ratingCount\")\n",
    "    return data\n",
    "\n",
    "# ---------- Collections from File ----------\n",
    "def read_collections(file_path):\n",
    "    \"\"\"Read collection names from txt file and return their URLs.\"\"\"\n",
    "    urls = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            name = line.strip()\n",
    "            if not name:\n",
    "                continue\n",
    "            slug = parse.quote(name.lower().replace(\" \", \"_\"))\n",
    "            urls.append(f\"{BASE}/food/collections/{slug}\")\n",
    "    return urls\n",
    "\n",
    "# ---------- Main Crawler ----------\n",
    "def crawl(max_collections=None, max_recipes_per_collection=None, \n",
    "          out_csv=\"bbc_food_recipes.csv\", out_jsonl=\"bbc_food_recipes.jsonl\"):\n",
    "    collections = read_collections(\"collections.txt\")\n",
    "    if max_collections:\n",
    "        collections = collections[:max_collections]\n",
    "    seen_urls = set()\n",
    "    total = 0\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csv_fp, \\\n",
    "         open(out_jsonl, \"w\", encoding=\"utf-8\") as jsonl_fp:\n",
    "        fields = [\"title\", \"url\", \"collection\", \"description\", \"servings\", \n",
    "                  \"total_time_min\", \"ingredients\", \"method\", \"tags\", \n",
    "                  \"ratingValue\", \"ratingCount\"]\n",
    "        writer = csv.DictWriter(csv_fp, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for i, coll_url in enumerate(collections, 1):\n",
    "            print(f\"[{i}/{len(collections)}] {coll_url}\")\n",
    "            page_url = coll_url\n",
    "            recipes_count = 0\n",
    "            while page_url:\n",
    "                html = fetch(page_url)\n",
    "                if not html:\n",
    "                    break\n",
    "                recipe_urls = get_links(html, \"/food/recipes/\")\n",
    "                for recipe_url in recipe_urls:\n",
    "                    if recipe_url in seen_urls:\n",
    "                        continue\n",
    "                    if max_recipes_per_collection and recipes_count >= max_recipes_per_collection:\n",
    "                        break\n",
    "                    data = parse_recipe(fetch(recipe_url) or \"\")\n",
    "                    data[\"url\"] = recipe_url\n",
    "                    data[\"collection\"] = coll_url\n",
    "                    writer.writerow({k: data.get(k) for k in fields})\n",
    "                    jsonl_fp.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                    seen_urls.add(recipe_url)\n",
    "                    recipes_count += 1\n",
    "                    total += 1\n",
    "                page_url = get_next_page(html)\n",
    "    print(f\"Done! Wrote {total} recipes to {out_csv} and {out_jsonl}\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"BBC Food Recipe Scraper\")\n",
    "    print(\"-\" * 40)\n",
    "    csv_file = \"bbc_food_recipes.csv\"\n",
    "    jsonl_file = \"bbc_food_recipes.jsonl\"\n",
    "    max_coll = None  # limit collections for testing\n",
    "    max_rec = 5   # limit recipes per collection for testing\n",
    "    crawl(max_collections=max_coll,\n",
    "          max_recipes_per_collection=max_rec,\n",
    "          out_csv=csv_file,\n",
    "          out_jsonl=jsonl_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be4a7ff",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19809012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 1.02 MB\n"
     ]
    }
   ],
   "source": [
    "# Scraped data\n",
    "data = pd.read_csv(\"bbc_food_recipes.csv\")\n",
    "size_bytes = os.path.getsize(\"bbc_food_recipes.csv\")\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"File size: {size_mb:.2f} MB\")\n",
    "# Load food list from Excel\n",
    "food_list = pd.read_excel(\"Frida_Dataset_May2025.xlsx\", sheet_name=\"Data_Table\", skiprows=1)\n",
    "\n",
    "food_list_units = food_list.iloc[0]\n",
    "\n",
    "# Remove rows 0 and 1, reset index, and rename first three columns\n",
    "food_list = (food_list.drop(index=[0, 1])\n",
    "             .reset_index(drop=True)\n",
    "             .rename(columns={food_list.columns[0]: \"Food_Danish\",\n",
    "                            food_list.columns[1]: \"Food_English\",\n",
    "                            food_list.columns[2]: \"Food_ID\"}))\n",
    "\n",
    "# Extract first part before comma from Food_English and get unique values\n",
    "single_food_list = list(set(item.split(',')[0] for item in food_list['Food_English']))\n",
    "\n",
    "# Add additional ingredients\n",
    "single_food_list.extend(['mushrooms', 'white wine', 'red wine', 'caster sugar', \n",
    "                        'chicken stock', 'vegetable stock', 'beef stock', 'basil', \n",
    "                        'oregano', 'thyme', 'rosemary', 'parmasan', 'mozzarella'])\n",
    "single_food_list = list(set(item.split(',')[0].strip().lower()\n",
    "                            for item in food_list['Food_English']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b8229",
   "metadata": {},
   "source": [
    "### Extract and apply clean list of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract and apply clean list of ingredients \n",
    "def clean_and_extract(ingredient_string, food_list):\n",
    "    if not isinstance(ingredient_string, str):\n",
    "        return []\n",
    "\n",
    "    # Lowercase and remove numbers + units\n",
    "    text = ingredient_string.lower()\n",
    "    text = re.sub(r\"\\d+[\\w/]*\", \"\", text)        # remove 30g, 1oz, 80ml etc.\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)         # remove punctuation\n",
    "    words = text.split()\n",
    "\n",
    "    # Basic singularization (very simple)\n",
    "    words = [w[:-1] if w.endswith(\"s\") else w for w in words]\n",
    "\n",
    "    matches = []\n",
    "    for w in words:\n",
    "        if w in food_list:\n",
    "            matches.append(w)\n",
    "\n",
    "    return list(set(matches))    # unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ec76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to dataset\n",
    "single_food_lower = [x.lower() for x in single_food_list]\n",
    "\n",
    "data[\"clean_ingredients\"] = data[\"ingredients\"].apply(\n",
    "    lambda x: clean_and_extract(x, single_food_lower)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recipes scraped: 445\n",
      "Total unique ingredients: 152\n"
     ]
    }
   ],
   "source": [
    "## Basic statistics\n",
    "print(f'Total recipes scraped: {len(data)}')\n",
    "print(f'Total unique ingredients: {len(set(item for sublist in data[\"final_ingredients\"] for item in sublist))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
